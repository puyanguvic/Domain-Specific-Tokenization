# Domain-Specific Tokenization (DST)

[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue.svg)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)]()
[![Reproducible](https://img.shields.io/badge/reproducible-‚úì-green.svg)]()

> A deterministic, grammar-guided tokenizer system for structured data ‚Äî 100 % reversible, linear-time, and drop-in compatible with Transformers.

---

## ‚ú® Overview

**Domain-Specific Tokenization (DST)** provides a *formal, efficient, and invertible* framework for encoding structured data such as HTTP logs, configuration files, source code, or biosequences.

It guarantees:
- ‚úÖ **Perfect round-trip fidelity** ‚Äì every input string can be exactly reconstructed.
- ‚öôÔ∏è **Deterministic finite-state encoding** ‚Äì compiled into DFSTs with $O(|x|)$ complexity.
- üß© **Grammar-aware vocabularies** ‚Äì guided by domain regular expressions and schemas.
- ü§ù **Hugging Face compatibility** ‚Äì exports `tokenizer.json` for existing Transformer stacks.

---

## üöÄ Quick Start

### 1Ô∏è‚É£ Install
```bash
git clone git@github.com:puyanguvic/Domain-Specific-Tokenization.git
cd Domain-Specific-Tokenization
pip install -e .
````

### 2Ô∏è‚É£ Train a tokenizer

```bash
dst train --input examples/sample_corpus.txt --output tokenizer.json
```

### 3Ô∏è‚É£ Encode / Decode in Python

```python
from dst.tokenizer import DSTTokenizer

corpus = ["GET /index.html HTTP/1.1", "Host: example.com"]
tokenizer = DSTTokenizer.train(corpus, min_freq=1)

tokens = tokenizer.encode("GET /index.html HTTP/1.1")
print(tokens)
# ['GET', ' ', '/', 'index', '.', 'html', ' ', 'HTTP', '/', '1', '.', '1']

print(tokenizer.decode(tokens))
# "GET /index.html HTTP/1.1"

assert tokenizer.verify(corpus)
```

---

## üìÇ Repository Structure

| Path                          | Description                                                                  |
| ----------------------------- | ---------------------------------------------------------------------------- |
| `dst/vocab.py`                | Grammar-guided vocabulary induction (regex extraction, frequency filtering). |
| `dst/dfst.py`                 | Deterministic finite-state transducer (DFST) encoder‚Äìdecoder.                |
| `dst/tokenizer.py`            | Main training / encoding / export interface.                                 |
| `dst/cli.py`                  | Command-line interface (`dst train`, `dst encode`).                          |
| `examples/sample_corpus.txt`  | Example HTTP corpus.                                                         |
| `tests/test_reversibility.py` | Unit test ensuring Œ∫(œÑ(x)) = x for all inputs.                               |

---

## ‚öôÔ∏è Command-Line Interface

```bash
usage: dst <command> [options]

Commands:
  train     Train a deterministic tokenizer from a text corpus
  encode    Encode text using a trained tokenizer
```

Examples:

```bash
# Train tokenizer
dst train --input examples/sample_corpus.txt --output tokenizer.json

# Encode a text file
dst encode --input examples/sample_corpus.txt --tokenizer tokenizer.json
```

---

## üß† Design Highlights

DST models tokenization as paired mappings between strings and token sequences:
[
\tau: \Sigma^* \to \mathcal{V}^*, \quad \kappa: \mathcal{V}^* \to \Sigma^*, \quad \kappa(\tau(x)) = x
]

It ensures:

* **Non-erasingness:** every token emits ‚â• 1 symbol.
* **Prefix-freeness:** unique segmentation, no ambiguity.
* **Bounded preimage:** finite inverse mappings ‚áí linear-time DFST.

The compiled automaton performs deterministic, auditable transformations suitable for large-scale enterprise or scientific data processing.

---

## üß™ Example Performance

| Property           | Value                                      |   |   |
| ------------------ | ------------------------------------------ | - | - |
| Reversibility      | ‚úÖ 100 %                                    |   |   |
| Avg Token Length   | ‚âà 4.2 chars                                |   |   |
| Sequence Reduction | 10‚Äì20 % vs Byte-BPE                        |   |   |
| Complexity         | O(                                         | x | ) |
| Export             | `tokenizer.json` (Hugging Face-compatible) |   |   |

---

## üìú License

Released under the **MIT License**
¬© 2025 Pu Yang

---

## üîó Related Work

* Sennrich et al., *Neural Machine Translation of Rare Words with Subword Units*, ACL 2016
* Xue et al., *ByT5: Towards a Token-Free Future with Byte-Level Models*, TACL 2022
* Ding et al., *Byte-Level Tradeoffs in Tokenization*, NeurIPS 2023