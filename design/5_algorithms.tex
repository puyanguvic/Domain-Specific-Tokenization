% Algorithm listings for DST (plain enumerate to avoid extra packages)

\subsection*{Algorithm 1: Grammar-Guided Vocabulary Induction}
\begin{enumerate}
  \item Input: corpus $\mathcal{D}$, base vocab $\mathcal{V}_0$, grammar filters $\mathcal{G}$, budget $B$.
  \item Compute fragmentation map under $\tau_0$; extract frequent $n$-grams ($2\!\le\!n\!\le\!8$) with high fragmentation and frequency.
  \item Apply grammar filters $\mathcal{G}$ (URLs, tags, protocol fields) to keep boundary-aligned candidates; insert into a trie with counts.
  \item Score each candidate by a weighted sum of: gradient salience, compression gain, MI alignment, and optional PPL change.
  \item Select top candidates by greedy/knapsack until $|\mathcal{V}|\le B$, ensuring prefix-free, non-erasing tokens.
  \item Return expanded vocabulary $\mathcal{V}$.
\end{enumerate}

\subsection*{Algorithm 2: Deterministic Encoderâ€“Decoder (DFST) Compilation}
\begin{enumerate}
  \item Build a prefix trie over $\mathcal{V}$; enforce prefix-free property via tie-breaking or token splitting.
  \item Emit encoder transitions by maximal munch: for each state and symbol, point to the longest matching continuation or a byte-fallback state.
  \item Emit decoder transitions as the exact inverse mapping; verify $\kappa(\tau(x))\!=\!x$ on a validation set.
  \item Densify transitions into contiguous arrays (state\,\times\,alphabet) and serialize.
\end{enumerate}

\subsection*{Algorithm 3: Vocabulary Expansion for Pretrained LMs}
\begin{enumerate}
  \item Initialize new embeddings by averaging constituent subtoken vectors or using cluster centroids.
  \item Freeze backbone for a short warm-up; train adapters or the embedding layer on mixed (domain/general) data.
  \item Unfreeze selectively; continue training with a small learning rate to avoid forgetting.
  \item Validate invertibility and general-language benchmarks; adjust candidate selection if degradation exceeds tolerance.
\end{enumerate}

