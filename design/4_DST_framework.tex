\section{DST Framework}
\label{sec:framework}

DST organizes tokenization into modular components with verifiable properties. The framework exposes a clean separation between symbolic priors (domain grammar) and statistical evidence (n‑gram counts), while compiling to deterministic finite-state artifacts for deployment.

\subsection{Components}

- \textbf{Normalizer.} Optional canonicalization constrained to be invertible within-domain (e.g., Unicode NFC, whitespace standardization without erasure). No operation may violate $\kappa\!\circ\!\tau = \mathrm{Id}$.
- \textbf{Vocabulary $\mathcal{V}$.} A set of strings over $\Sigma$ satisfying grammar constraints and learned frequency thresholds (\S\ref{sec:method-vocab}). Byte-fallback tokens guarantee coverage.
- \textbf{Encoder $\tau$.} A deterministic greedy maximal‑munch transduction realized by a trie/DFST (\S\ref{sec:method-fst}).
- \textbf{Decoder $\kappa$.} A deterministic inverse that concatenates literal token surfaces; identical code path across domains.
- \textbf{Exporter.} Serialization to \texttt{tokenizer.json} with a transition table and metadata for integration in standard LLM stacks (\S\ref{sec:method-export}).

\subsection{Guarantees}

- \textbf{Exact consistency.} $\kappa(\tau(x))=x$ for all valid inputs.
- \textbf{Finite-state realizability.} Subsequence‑deterministic DFST without $\epsilon$‐edges, linear in input length.
- \textbf{Domain awareness.} Grammar filters prevent cross-boundary merges (e.g., mixing identifiers and delimiters in code).
- \textbf{Coverage.} Byte fallback maintains invertibility for out‑of‑vocabulary symbols.

\subsection{Grammar Priors}

Grammar priors are expressed as regular expressions or token classes, e.g.,

- identifiers: `[A-Za-z_][A-Za-z0-9_]*`,
- numerics: `[0-9]+(\.[0-9]+)?`,
- delimiters/operators: domain‑specific lists,
- markup tags or headers: simplified patterns,
- biological k‑mers: fixed‑length token families.

Priors act as filters on candidate n‑grams and as hard constraints during DFST compilation, ensuring non‑erasing and concatenative behavior.

\subsection{Interfaces and Integration}

The exported artifacts interoperate with Hugging Face tokenizers and Transformers APIs. A light shim maps DFST transitions to the runtime’s encode/decode interface, enabling fair comparisons to BPE/Unigram baselines without modifying models.

\subsection{Safety and Auditing}

DST’s determinism and invertibility facilitate audit trails: every decoded sequence has an exact preimage. This property simplifies dataset forensics, reproduction, and alignment with privacy filters that require precise redaction behavior.
