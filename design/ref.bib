@inproceedings{gage1994bpe,
  title={A New Algorithm for Data Compression},
  author={Gage, Philip},
  booktitle={The C Users Journal},
  year={1994},
  note={Byte Pair Encoding}
}

@inproceedings{wu2016google,
  title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
  author={Wu, Yonghui and others},
  booktitle={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@inproceedings{kudo2018sentencepiece,
  title={SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  author={Kudo, Taku and Richardson, John},
  booktitle={EMNLP: System Demonstrations},
  year={2018}
}

@inproceedings{bostrom2020byte,
  title={Byte Pair Encoding is Suboptimal for Language Model Pretraining},
  author={Bostrom, Kaj and Durrett, Greg},
  booktitle={ACL},
  year={2020}
}

@inproceedings{xue2022byt5,
  title={ByT5: Towards a token-free future with pre-trained byte-to-byte models},
  author={Xue, Linting and others},
  booktitle={TACL},
  year={2022}
}

@inproceedings{clark2022canine,
  title={CANINE: Pre-Training an Efficient Tokenization-Free Encoder for Language Representation},
  author={Clark, Jonathan H. and others},
  booktitle={TACL},
  year={2022}
}

@inproceedings{gao2023representation,
  title={Representation Matters for Tokenization},
  author={Gao, {\relax et al.}},
  booktitle={arXiv preprint},
  year={2023},
  note={verify}
}

@inproceedings{radford2019gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and others},
  booktitle={OpenAI Technical Report},
  year={2019}
}

@inproceedings{tay2022charformer,
  title={Charformer: Fast Character Transformers via Gradient-Based Subword Tokenization},
  author={Tay, Yi and others},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{taylor2021learned,
  title={Learned Subword Tokenization in Neural Language Models},
  author={Taylor, {\relax et al.}},
  booktitle={arXiv preprint},
  year={2021},
  note={verify}
}

@inproceedings{kaufman2023soft,
  title={Soft Tokenization for Language Models},
  author={Kaufman, {\relax et al.}},
  booktitle={arXiv preprint},
  year={2023},
  note={verify}
}

@inproceedings{wang2023neural,
  title={Neural Tokenizers: Differentiable Tokenization for End-to-End Training},
  author={Wang, {\relax et al.}},
  booktitle={arXiv preprint},
  year={2023},
  note={verify}
}

@inproceedings{huang2023adapttok,
  title={AdaptTok: Adaptive Tokenization for Efficient Language Modeling},
  author={Huang, {\relax et al.}},
  booktitle={arXiv preprint},
  year={2023},
  note={verify}
}

@inproceedings{feng2020codebert,
  title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  author={Feng, Zhangyin and others},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{wang2021codet5,
  title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author={Wang, Yue and others},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{ahmad2021unified,
  title={Unified Pre-Training for Program Understanding and Generation},
  author={Ahmad, Wasi and others},
  booktitle={NAACL},
  year={2021}
}

@inproceedings{ji2021dnabert,
  title={DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome},
  author={Ji, Yanrong and others},
  booktitle={Bioinformatics},
  year={2021}
}

@inproceedings{zhou2023proteinbert,
  title={ProteinBERT: A universal deep-learning model of protein sequence and function},
  author={Zhou, {\relax et al.}},
  booktitle={Bioinformatics},
  year={2023},
  note={verify}
}

@inproceedings{han2023mathbert,
  title={MathBERT: A Pre-trained Language Model for Mathematical Text},
  author={Han, {\relax et al.}},
  booktitle={arXiv preprint},
  year={2023},
  note={verify}
}

@article{mohri1997finite,
  title={Finite-State Transducers in Language and Speech Processing},
  author={Mohri, Mehryar},
  journal={Computational Linguistics},
  year={1997}
}

@inproceedings{mohri2004weighted,
  title={Weighted Finite-State Transducers in Speech Recognition},
  author={Mohri, Mehryar and Pereira, Fernando and Riley, Michael},
  booktitle={Computer Speech and Language},
  year={2004}
}

@book{beesley2003finite,
  title={Finite State Morphology},
  author={Beesley, Kenneth R. and Karttunen, Lauri},
  year={2003},
  publisher={CSLI}
}

@inproceedings{choffrut1979sequential,
  title={A Characterization of Sequential Transducers},
  author={Choffrut, Christian},
  booktitle={Theoretical Computer Science},
  year={1979}
}

@inproceedings{wei2025tokenizer,
  title={Tokenizer Consistency: When Does Tokenization Preserve Statistical Estimation?},
  author={Wei, {\relax et al.}},
  booktitle={preprint},
  year={2025},
  note={verify}
}

@article{brown1992statistical,
  title={Class-based n-gram Models of Natural Language},
  author={Brown, Peter F. and others},
  journal={Computational Linguistics},
  year={1992}
}

@article{shannon1948mathematical,
  title={A Mathematical Theory of Communication},
  author={Shannon, Claude E.},
  journal={Bell System Technical Journal},
  year={1948}
}

@inproceedings{ramesh2021tokenization,
  title={On the Impact of Tokenization in Language Modeling},
  author={Ramesh, {\relax et al.}},
  booktitle={arXiv preprint},
  year={2021},
  note={verify}
}

@inproceedings{liu2024tokeneval,
  title={TokenEval: Benchmarking Tokenization for Language Models},
  author={Liu, {\relax et al.}},
  booktitle={arXiv preprint},
  year={2024},
  note={verify}
}
