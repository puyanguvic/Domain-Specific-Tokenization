\begin{abstract}
Tokenization is the interface between raw symbolic inputs and neural representations. Subword methods are effective for natural language but often cross domain boundaries and lack invertibility in structured settings, compromising estimator correctness and auditability. We introduce \emph{Domain-Specific Tokenization} (DST), a framework that treats tokenization as paired mappings $(\tau,\kappa)$ and establishes that estimator consistency in token space is equivalent to exact reversibility in the original domain ($\kappa\!\circ\!\tau = \mathrm{Id}$). We further derive sufficient conditions—multiplicativity and non‑erasingness—under which the tokenizer compiles to deterministic finite‑state transducers with linear‑time encoding and decoding.

DST integrates lightweight grammar priors into vocabulary induction to respect domain boundaries while retaining full byte coverage, and exports to standard tokenizer formats for drop‑in use with contemporary Transformer stacks. In preliminary evaluations on structured domains (e.g., source code, configuration files, protocols, and biosequences), DST achieves 100\% round‑trip reconstruction and reduces sequence length by 10–20\% relative to subword baselines, with lower token‑entropy dispersion. These properties position DST as a practical, verifiable tokenization layer for domain‑aware language modeling.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Tokenization is the interface between raw symbolic inputs and neural representations. It determines the modeling units, vocabulary, and the path back to the original string, and therefore affects efficiency, generalization, and interpretability \citep{bostrom2020byte, xue2022byt5, clark2022canine, gao2023representation}. While subword methods such as BPE \citep{gage1994bpe}, WordPiece \citep{wu2016google}, and Unigram \citep{kudo2018sentencepiece} work well for natural language, they degrade in structured or semi-formal domains—source code \citep{feng2020codebert, wang2021codet5}, configuration files, network protocols, or genomic sequences \citep{ji2021dnabert, zhou2023proteinbert}. In these settings, heuristics often cross syntactic boundaries, rely on non-invertible normalization, and fail to guarantee $\kappa(\tau(x)) = x$, undermining estimator correctness and auditability.

This paper argues that tokenization should be treated as a first-class modeling component with two non-negotiable properties: (i) \emph{consistency}—every valid input is exactly reconstructable ($\kappa\!\circ\!\tau = \mathrm{Id}$); and (ii) \emph{finite-state realizability}—encoder and decoder are deterministic and linear-time \citep{mohri1997finite}. Existing mainstream tokenizers typically satisfy at most one of these.

In professional, domain-specific applications, we further contend that the role of the tokenizer is underestimated: a carefully optimized tokenizer can deliver fewer tokens, higher computational efficiency, and better semantic preservation. These benefits translate into shorter sequences for the same content, lower memory and latency at training and inference, and faithful round-trip reconstruction that preserves task-critical structure.

We introduce \textbf{Domain-Specific Tokenization (DST)}, a framework that unifies theory and practice for structured inputs. DST (i) formalizes tokenization as paired mappings $(\tau,\kappa)$ and proves that estimator consistency in token space is equivalent to reversibility in the original domain (\S\ref{sec:theory}); (ii) derives sufficient conditions—multiplicativity and non-erasingness—under which the tokenizer compiles to deterministic finite-state transducers with linear complexity (\S\ref{sec:method}); and (iii) integrates domain grammar priors into vocabulary induction to respect boundaries while retaining byte-level coverage and export to standard tooling (\S\ref{sec:framework}).

Empirically, DST preserves perfect invertibility across multiple structured domains and reduces sequence length compared to subword baselines, yielding lower entropy dispersion and practical speedups. The framework exports to \texttt{tokenizer.json} for drop-in use with contemporary Transformer stacks, positioning tokenization as an analyzable, verifiable, and learnable module in domain-aware language modeling.

Compatibility with general-language capabilities is evaluated via standard benchmarks to track forgetting; our setup maintains strong performance while delivering domain gains.

\section{Related Work}
\label{sec:related}

\subsection{Tokenization Paradigms}

Subword methods—BPE \citep{gage1994bpe}, WordPiece \citep{wu2016google}, and Unigram \citep{kudo2018sentencepiece}—segment by frequency-driven merges; SentencePiece and byte variants \citep{radford2019gpt2} extend to multilingual and byte-level inputs. Character/byte models (ByT5 \citep{xue2022byt5}, CANINE \citep{clark2022canine}, Charformer \citep{tay2022charformer}) remove segmentation but expand sequence length and reduce semantic compression. Learned or adaptive tokenizers \citep{taylor2021learned, kaufman2023soft, wang2023neural, huang2023adapttok} co-train segmentation with models, improving flexibility but complicating verification and deployment. In structured domains—code \citep{feng2020codebert, wang2021codet5, ahmad2021unified}, biosequences \citep{ji2021dnabert, zhou2023proteinbert}, mathematical text \citep{han2023mathbert}—ad‑hoc rules or parsers are common, yet most approaches do not guarantee exact invertibility.

\subsection{Consistency, Finite-State Realizability, and Information Criteria}

Reversibility provides a clean statistical criterion: estimator consistency in token space lifts to the original domain if and only if decoding inverts encoding \citep{wei2025tokenizer}. Finite‑state methods \citep{mohri1997finite, mohri2004weighted, beesley2003finite} offer deterministic, linear‑time realizations for such mappings. Under multiplicativity and non‑erasingness, tokenizer pairs admit subsequential FST implementations \citep{choffrut1979sequential}. From an information perspective, suboptimal segmentation inflates entropy and distorts representations \citep{brown1992statistical, shannon1948mathematical, bostrom2020byte, ramesh2021tokenization, liu2024tokeneval}. DST situates tokenization as an information‑preserving, finitely realizable mapping that respects domain structure while remaining practical for modern LMs.

\section{Theory and Methodology}
\label{sec:theory}

In this section we formalize tokenization as a pair of mappings between
string and token spaces, derive a fundamental theorem connecting
reversibility and estimator consistency, and establish finite-state
conditions that make such mappings efficiently realizable.

% ------------------------------------------------------------
\subsection{Tokenizer as a Pair of Stochastic Maps}

Let $\Sigma$ denote a finite alphabet and
$\mathcal{X}=\Sigma^{*}$ the set of all finite strings.
A tokenizer is characterized by two measurable mappings:
\[
\tau:\mathcal{X}\to\mathcal{Y},\qquad
\kappa:\mathcal{Y}\to\mathcal{X},
\]
where $\mathcal{Y}=\mathcal{V}^{*}$ is the space of token sequences over a
finite vocabulary $\mathcal{V}$.
The encoder $\tau$ converts strings into token sequences,
while the decoder $\kappa$ reconstructs strings from tokens.
Both may be deterministic or stochastic.

For a distribution $P$ over $\mathcal{X}$, we denote its
pushforward under $\tau$ as
$\tau_{\#}P(A)=P(\{x:\tau(x)\in A\})$.
When a model learns a distribution $\hat Q$ on $\mathcal{Y}$,
the induced distribution on $\mathcal{X}$ is
$\kappa_{\#}\hat Q$.

% ------------------------------------------------------------
\subsection{Exact and Statistical Consistency}

\begin{definition}[Exact Consistency]
A tokenizer pair $(\tau,\kappa)$ is \emph{exactly consistent}
if and only if
\[
\forall x\in\mathcal{X},\qquad
\kappa(\tau(x))=x.
\]
\end{definition}

\begin{definition}[Statistical Consistency]
Let $\{\hat Q_n\}$ be a sequence of estimators on $\mathcal{Y}$ such that
$\hat Q_n\xrightarrow{n\to\infty}\tau_{\#}P$.
We call $(\tau,\kappa)$ \emph{statistically consistent}
if $\kappa_{\#}\hat Q_n \to P$ in distribution.
\end{definition}

\begin{theorem}[Fundamental Consistency Theorem]
\label{thm:consistency}
For all $P\in\mathcal{P}(\mathcal{X})$ and all estimator sequences
$\{\hat Q_n\}$ consistent on $\mathcal{Y}$,
the induced estimators $\kappa_{\#}\hat Q_n$
converge to $P$ if and only if
\[
\kappa\!\circ\!\tau = \mathrm{Id}_{\mathcal{X}}.
\]
\end{theorem}

\begin{proof}[Sketch]
($\Rightarrow$)  
Assume $\kappa\!\circ\!\tau = \mathrm{Id}$.  
For any bounded test function $f$,
\[
\int f\,d(\kappa_{\#}\hat Q_n)
=\int f(\kappa(y))\,d\hat Q_n(y)
\to \int f(\kappa(y))\,d(\tau_{\#}P)(y)
= \int f(x)\,dP(x).
\]

($\Leftarrow$)  
Take $P=\delta_x$ (Dirac at $x$).
Then $\tau_{\#}P=\delta_{\tau(x)}$ and
statistical consistency gives
$\kappa_{\#}\delta_{\tau(x)}=\delta_x$,
so $\kappa(\tau(x))=x$.
\end{proof}

Hence, reversibility is both necessary and sufficient
for lifting estimator convergence from token space
back to the original domain.
Consistency is therefore not a heuristic preference
but a \emph{theoretical prerequisite} for faithful modeling.

% ------------------------------------------------------------
\subsection{Multiplicativity and Non-Erasing Constraints}

Exact consistency alone does not guarantee computational feasibility.
To ensure linear-time realizability we impose two structural constraints:

\begin{enumerate}
\item \textbf{Multiplicativity.}
For all $x_1,x_2\in\mathcal{X}$,
\[
\tau(x_1x_2)=\tau(x_1)\tau(x_2).
\]
This enforces concatenative composition—tokenizing a
concatenation equals concatenating tokenizations.

\item \textbf{Non-Erasing Property.}
For every symbol $a\in\Sigma$, $\tau(a)\neq\epsilon$,
so that no input character disappears.
\end{enumerate}

Together these imply that $\tau$
is a \emph{monoid homomorphism} from
$(\Sigma^{*},\cdot)$ to $(\mathcal{V}^{*},\cdot)$,
ensuring the encoder respects sequence concatenation.

\begin{proposition}[Deterministic Transduction]
If $\tau$ is multiplicative and non-erasing,
then $(\tau,\kappa)$ can be implemented as
a pair of deterministic finite-state transducers (DFSTs)
of size $O(|\mathcal{V}|\,|\Sigma|)$.
\end{proposition}

\begin{proof}[Idea]
Construct a trie $\mathcal{T}$ of all tokens in $\mathcal{V}$.
Each node corresponds to a prefix; terminal nodes emit token IDs.
A greedy “maximal-munch’’ traversal defines $\tau$.
Because no $\epsilon$-edges occur and every prefix is unique,
the mapping is subsequential \citep{choffrut1979sequential}.
The inverse $\kappa$ concatenates literal outputs.
\end{proof}

This establishes that the consistency condition
can be satisfied with deterministic, linearly scalable automata.

% ------------------------------------------------------------
\subsection{Ambiguity and Bounded Variation}

Ambiguity arises if different token sequences map to the same string,
i.e.\ $\exists y_1\ne y_2$ such that $\kappa(y_1)=\kappa(y_2)$.
Let
\[
A(x)=|\{y\in\mathcal{Y}:\kappa(y)=x\}|
\]
denote the \emph{ambiguity index}.
For consistent deterministic tokenizers $A(x)=1$ for all $x$.
Bounded-variation results \citep{mohri1997finite}
show that if $A(x)$ is finite and $\tau$ is length-monotone,
then a subsequential realization exists.
DST enforces $A(x)\!\equiv\!1$ by design.

When stochastic variants are desired,
probabilities can be marginalized as
\[
P(x)=\sum_{y\in\tau^{-1}(x)}P(y)
\approx\sum_{y\in\mathrm{TopK}(\tau^{-1}(x))}P(y),
\]
yielding tractable approximations while preserving reversibility
for the dominant paths.

% ------------------------------------------------------------
\subsection{Information-Theoretic Interpretation}

Let $H_{\text{char}}$ and $H_{\text{tok}}$
denote the empirical entropies of character
and token sequences respectively.
A consistent tokenizer preserves mutual information:
\[
I(X;Y)=H_{\text{char}}-H_{\text{char}\mid Y}=H_{\text{tok}}.
\]
Inconsistent tokenizers induce
$H_{\text{char}\mid Y}>0$,
reflecting information loss.
From a compression perspective,
DST aims to minimize redundancy
subject to perfect invertibility:
\[
\min_{\tau}\; H_{\text{tok}} \quad
\text{s.t.}\quad \kappa\!\circ\!\tau = \mathrm{Id}.
\]
This connects consistency to Shannon-optimal coding:
any reversible minimal-entropy mapping is
an information-preserving representation.

% ------------------------------------------------------------
\subsection{Implications}

\begin{itemize}
\item The theorem provides a unified criterion for evaluating
tokenizers across languages, codes, or protocols.
\item The multiplicativity and non-erasing constraints ensure
linear-time deterministic implementation.
\item The information-theoretic view explains why consistent
tokenization improves compression and model generalization.
\end{itemize}

Thus, Domain-Specific Tokenization (DST)
links theoretical consistency, finite-state computability,
and representational efficiency under a single framework.


\section{Methodology}
\label{sec:method}

This section translates the theoretical guarantees from
Section \ref{sec:theory} into a practical construction pipeline.
Domain-Specific Tokenization (DST) operationalizes
consistency and finite-state realizability through three stages:
(1) vocabulary induction with domain awareness,
(2) deterministic encoder–decoder compilation,
and (3) optional probabilistic marginalization for ambiguous cases.

% ------------------------------------------------------------
\subsection{Design Philosophy}
\label{sec:method-design}

DST follows three design tenets that jointly ensure
reversibility, efficiency, and generality:

\begin{enumerate}
  \item \textbf{Consistency as a constraint.}
        Every transformation must satisfy
        $\kappa(\tau(x))\!=\!x$ for all $x$
        within the domain grammar. No detokenization heuristics
        or post-hoc normalization are allowed.

  \item \textbf{Determinism through maximal munch.}
        The encoder scans the input and always chooses
        the longest token prefix present in the vocabulary.
        This yields a subsequential deterministic mapping
        and prevents overlapping ambiguities.

  \item \textbf{Domain awareness via grammar priors.}
        Candidate tokens are filtered through
        regular-expression-style patterns that express
        permissible symbols and delimiters
        of the structured domain.
        This hybrid symbolic–statistical design
        generalizes across programming languages,
        markup formats, logs, and bio-sequences.
\end{enumerate}

% ------------------------------------------------------------
\subsection{Stage I – Domain-Aware Vocabulary Induction}
\label{sec:method-vocab}

The first stage builds a compact vocabulary $\mathcal{V}$
that covers the corpus while respecting its syntactic grammar.
Unlike pure frequency methods (e.g., BPE),
DST constructs $\mathcal{V}$ via the following algorithm.

We compute n-gram counts up to length $N$, retain candidates that satisfy domain grammar constraints, and augment with byte-fallback tokens to ensure coverage. This yields a vocabulary that respects boundaries while capturing frequent substrings. The procedure runs in $O(|\mathcal{D}|N)$ time with shard-parallel counting; memory scales with the number of retained candidates.

% ------------------------------------------------------------
\subsection{Stage II – Finite-State Encoder and Decoder}
\label{sec:method-fst}

Given $\mathcal{V}$, DST compiles two deterministic finite-state
transducers (DFSTs): the encoder $\tau$ and its inverse $\kappa$.
A trie $\mathcal{T}$ over $\mathcal{V}$ is constructed once;
each node corresponds to a prefix, terminal nodes mark tokens.

Given $\mathcal{V}$, we build a trie over all token strings and compile a deterministic transition table. Encoding performs greedy maximal‑munch traversal; decoding concatenates token surfaces. Build time is $O(|\mathcal{V}|)$ and runtime is $O(|x|)$ with no backtracking.

\paragraph{Complexity and Implementation.}
The trie is compiled into a transition table
of dimension $|Q|\!\times\!|\Sigma|$,
each entry storing the next-state index and optional token ID.
Runtime complexity is $O(|x|)$ per sequence;
memory usage is linear in $|\mathcal{V}|$.
Such tables map cleanly to GPU kernels or C++ inference code
with constant-time lookups.

\paragraph{Correctness Properties.}
\begin{itemize}
  \item Deterministic encoding and decoding (no backtracking)
  \item Non-erasing transitions (no $\epsilon$ edges)
  \item $\kappa(\tau(x))\!=\!x$ for all $x$
  \item Linear complexity and bounded latency
\end{itemize}

These satisfy the conditions of Theorem \ref{thm:consistency}
and Proposition 3.1, guaranteeing both
statistical and computational consistency.

% ------------------------------------------------------------
\subsection{Stage III – Probabilistic Marginalization (Optional)}
\label{sec:method-kbest}

In domains with overlapping patterns,
DST approximates the marginal likelihood $P(x)$
over all tokenizations by a top-$K$ beam search:

For optional probabilistic scoring in domains with overlapping patterns, we approximate $P(x)$ via a top-$K$ beam over valid DFST paths with complexity $O(K|x|)$. Small beams ($K\!\le\!20$) typically suffice, and exact invertibility is preserved on dominant paths.

% ------------------------------------------------------------
\subsection{Export and Integration}
\label{sec:method-export}

DST tokenizers are serialized into the
\texttt{tokenizers} library schema used by Hugging Face:

\begin{verbatim}
tokenizer.json
tokenizer_config.json
vocab.txt
\end{verbatim}

Each file encodes DFST transitions, normalizers,
and byte-fallback tables.
Models can thus invoke DST transparently through
\texttt{AutoTokenizer.from_pretrained()}.

\paragraph{Complexity Profile.}
\begin{center}
\begin{tabular}{lccc}
\toprule
Stage & Time Complexity & Space Complexity & Determinism \\
\midrule
Vocabulary Induction & $O(|\mathcal{D}|N)$ & $O(|\mathcal{V}|)$ & ✔ \\
DFST Compilation & $O(|\mathcal{V}|)$ & $O(|Q||\Sigma|)$ & ✔ \\
Encoding/Decoding & $O(|x|)$ & $O(1)$ per symbol & ✔ \\
Marginalization ($K$) & $O(K|x|)$ & $O(K)$ & stochastic approx. \\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Visualization.}
\begin{figure}[h]
\centering
% Guard include in case the figure is not yet available
\IfFileExists{figures/dst_pipeline.pdf}{%
  \includegraphics[width=0.9\textwidth]{figures/dst_pipeline.pdf}%
}{%
  \fbox{DST pipeline figure placeholder}%
}
\caption{DST pipeline: (1) Corpus → (2) Vocabulary → (3) DFST Encoder/Decoder → (4) Optional Marginalization → (5) Model Integration.}
\label{fig:dst_pipeline}
\end{figure}

% ------------------------------------------------------------
\subsection{Discussion}
DST’s construction provides a general recipe for
\emph{finite-state, consistent, domain-aware tokenization}.
It separates symbolic knowledge (grammars, delimiters)
from statistical estimation (frequency counts)
while ensuring that the overall mapping
remains mathematically and computationally sound.

% ------------------------------------------------------------
\subsection{Domain-Specific Vocabulary Expansion for Pretrained LMs}
\label{sec:method-dsv}

Beyond building a consistent tokenizer from first principles, many applications begin with a general LLM and its base vocabulary $\mathcal{V}_0$ (e.g., BPE/WordPiece for BERT, T5, Llama), and a domain corpus $\mathcal{D}_e$ (e.g., HTTP traces, HTML, finance/legal text). We formalize a practical, three-stage pipeline for expanding $\mathcal{V}_0$ while preserving consistency and compatibility.

\paragraph{Goal.} Find an expansion $\mathcal{V}\supseteq\mathcal{V}_0$ and a consistent encoder–decoder $(\tau,\kappa)$ that (i) compresses domain inputs efficiently (shorter sequences), (ii) aligns tokens with domain semantic units (e.g., “\texttt{<script>}”, “\texttt{.exe}”), and (iii) retains general-text performance (avoids catastrophic forgetting), with optional adaptivity over time.

\paragraph{Stage 1: Corpus analysis and candidate extraction.} Tokenize $\mathcal{D}_e$ with the base tokenizer $\tau_0$ and measure fragmentation. For a candidate string $w$, define its fragmentation factor $F(w)=\mathbb{E}[|\tau_0(w)|]$ over occurrences; high $F(w)$ indicates poor coverage. Extract frequent $n$-grams (e.g., $n\in[2,8]$) with frequency $f(u)>\theta$, filter out strings already represented as single tokens in $\mathcal{V}_0$, and optionally apply regex-style grammar filters (URLs, HTML tags, protocol fields). Organize candidates in a trie to enable efficient aggregation.

\paragraph{Stage 2: Importance scoring and selection.} We consider complementary signals:
\begin{itemize}
  \item \textbf{Gradient signal (VEGAD-style).} Run a short forward–backward pass of the base model on $\mathcal{D}_e$ without full fine-tuning; compute token-level gradient norms $G_t=\|\nabla_E L(x_t)\|_2$ at the embedding layer and aggregate to a candidate $w$ via the trie: $G_w=\sum_{t\in T(w)}G_t$.
  \item \textbf{Compression gain.} Measure the fractional reduction in total tokens when adding $w$: $\Delta C_w=\frac{|\tau_0(\mathcal{D}_e)|-|\tau_1(\mathcal{D}_e)|}{|\tau_0(\mathcal{D}_e)|}$, where $\tau_1$ is $\tau_0$ augmented with $w$.
  \item \textbf{Language-model improvement.} Optionally estimate perplexity change $\Delta\mathrm{PPL}_w$ on held-out domain text using a lightweight adaptation step.
  \item \textbf{Mutual-information proxy.} Use PMI/MI to favor boundary-aligned units under the domain grammar.
\end{itemize}
Combine signals into a normalized score
\[
\mathrm{Score}(w)=\alpha\,\widetilde{G}_w+\beta\,\widetilde{\Delta C}_w+\gamma\,\widetilde{I}_w+\eta\,\widetilde{\Delta\mathrm{PPL}}_w\,.
\]
Select top-$K$ or solve a knapsack-style subset under a vocabulary budget with objective
\[
\max_{\mathcal{V}\subseteq\mathcal{C}}\; U(\mathcal{V})=\alpha\,\Delta C(\mathcal{V})+\beta\,\Delta\mathrm{PPL}(\mathcal{V})+\gamma\,G(\mathcal{V})-\lambda|\mathcal{V}|,\quad \text{s.t. consistency.}
\]
Greedy selection with lazy updates is effective in practice.

\paragraph{Stage 3: Vocabulary expansion and model adaptation.} Merge selected candidates into the tokenizer while maintaining non-erasingness and maximal-munch determinism. Expand the embedding matrix from $\mathbb{R}^{|\mathcal{V}_0|\times d}$ to $\mathbb{R}^{|\mathcal{V}|\times d}$. Initialize new embeddings via (i) mean of constituent subtokens, (ii) cluster centroids, or (iii) small Gaussian noise followed by brief domain adaptation. Continue self-supervised training (MLM for encoder-only; causal LM/SFT for decoder/seq2seq) on a mixture of general and domain text to mitigate forgetting. Export the updated tokenizer as DFST-backed artifacts for deterministic deployment.

\paragraph{Compatibility and safety.} Maintain a byte-level fallback path for unseen symbols; validate that $\kappa\!\circ\!\tau=\mathrm{Id}$ over a domain-heldout set and general-text samples. Track regressions on generic benchmarks to monitor cross-domain performance.
This property allows DST to serve as
a unifying front-end for models trained on
structured or semi-structured data,
ranging from programming languages and logs
to chemical and genomic sequences.

\section{Experimental Setup}
\label{sec:setup}

We evaluate DST across structured domains against subword and character/byte baselines under matched budgets and training protocols.

\subsection{Domains and Datasets}

- \textbf{Source code:} Python/Java snippets from public repositories; de-identified variable names where required.
- \textbf{Configuration and logs:} Kubernetes YAML, Nginx config, and anonymized service logs.
- \textbf{Protocols:} HTTP request traces and simplified packet payloads.
- \textbf{Biosequences:} DNA (k-mers) and protein strings.
- \textbf{General text (compatibility):} Lightweight general-language tasks to assess forgetting.

\subsection{Baselines}

- BPE \citep{gage1994bpe}, WordPiece \citep{wu2016google}, Unigram \citep{kudo2018sentencepiece}
- Byte-BPE \citep{radford2019gpt2}, character/byte models \citep{xue2022byt5, clark2022canine}

\subsection{Metrics}

We focus on the trio: fewer tokens, higher efficiency, better semantic preservation.

- \textbf{Fewer tokens:} average tokens per sequence; tokens per byte/character.
- \textbf{Higher efficiency:} throughput (tokens/s), step latency (ms/step), memory footprint (GB) at fixed batch/context.
- \textbf{Semantic preservation:} invertibility rate (target 100\%), boundary violations per sequence, reconstruction error rate.
- \textbf{Compression quality:} token and n-gram entropy.
- \textbf{Downstream:} task metrics (e.g., perplexity, accuracy/F1).
- \textbf{Compatibility/Forgetting:} score deltas on general benchmarks (e.g., MMLU/HellaSwag).

\subsection{Setup}

We train vocabularies with comparable size budgets (e.g., 16k/32k). Models are small/medium Transformers trained for parity across tokenizers. Normalization and byte coverage are controlled for fairness.

\subsection{Measurement Protocol}

Throughput/latency are measured on a fixed hardware profile (e.g., A100 40GB) with identical batch and context length. Memory is peak allocated VRAM. Boundary violations are computed from regex-based grammars (tags, identifiers, protocol fields). Invertibility is exact string equality after round-trip.

\section{Results and Analysis}
\label{sec:results}

\subsection{Main Results}

\begin{table}[h]
  \centering
  \small
  \begin{tabular}{lcccccc}
    \toprule
    Tokenizer & Invertible & Len. $\downarrow$ & Ent. $\downarrow$ & Thru $\uparrow$ & Mem $\downarrow$ & Downstream $\uparrow$ \\
    \midrule
    BPE &  \;no &  -- & -- & -- & -- & -- \\
    WordPiece & \;no & -- & -- & -- & -- & -- \\
    Unigram & \;no & -- & -- & -- & -- & -- \\
    Byte‑BPE &  yes & -- & -- & -- & -- & -- \\
    DST (ours) &  yes & -- & -- & -- & -- & -- \\
    \bottomrule
  \end{tabular}
  \caption{Cross-domain summary. We report the trio (fewer tokens, higher efficiency, better semantic preservation). Numbers to be filled after training.}
  \label{tab:main}
\end{table}

\subsection{Case Studies}

DST respects identifiers and delimiters in code/logs and preserves structured substrings (e.g., tags, protocol fields), enabling faithful de-/re-identification and reducing boundary violations relative to subword baselines.

\subsection{Reproducibility}

We will release scripts for vocabulary induction, DFST compilation, and training configurations. Artifacts export to \texttt{tokenizer.json} for drop-in use with common LLM toolchains.

\section{Ablation and Discussion}
\label{sec:ablation}

\subsection{Ablations}

- Grammar priors (on/off) and boundary-violation impact
- Max n-gram $N$ and frequency threshold $\theta$
- Byte fallback toggling and rare-symbol coverage
- Scoring weights $(\alpha,\beta,\gamma,\eta)$ and vocabulary budget
- Embedding initialization strategies and adaptation schedule

\subsection{Discussion}

- \textbf{Fewer tokens.} Boundary-aware vocabulary induction shortens sequences while preserving coverage via byte fallback.
- \textbf{Higher efficiency.} Deterministic DFST encoding/decoding and shorter sequences increase throughput and reduce memory/latency.
- \textbf{Semantic preservation.} Exact round-trip and fewer boundary violations maintain task-critical structure; improves auditability.
- \textbf{Compatibility.} Mixed-domain adaptation maintains general-language performance; forgetting is quantified via benchmark deltas.

\section{Conclusion and Future Work}
\label{sec:conclusion}

We presented Domain-Specific Tokenization (DST), a consistent and finitely realizable tokenization framework for structured domains. By enforcing reversibility and compiling to deterministic finite-state transducers, DST delivers the trio of benefits—fewer tokens, higher efficiency, and better semantic preservation—while remaining compatible with modern LLM tooling.

Future directions include adaptive vocabulary updates driven by gradient/compression signals, domain-mixture tokenizers with shared bases and domain-specific extensions, and character-hybrid modes for highly structured substrings. We also plan to broaden evaluations and study representation bias and fairness under consistent tokenization.

