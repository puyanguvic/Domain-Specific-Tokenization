\section{Experiments}
\label{sec:experiments}

We evaluate DST across structured domains, comparing against standard subword baselines and character/byte models.

\subsection{Domains and Datasets}

- \textbf{Source code:} Python/Java snippets from public repositories; de‑identified variable names where required.
- \textbf{Configuration and logs:} Kubernetes YAML, Nginx config, and anonymized service logs.
- \textbf{Protocols:} HTTP request traces and simplified packet payloads.
- \textbf{Biosequences:} DNA (k‑mers) and protein strings.

\subsection{Baselines}

- BPE \citep{gage1994bpe}, WordPiece \citep{wu2016google}, Unigram \citep{kudo2018sentencepiece}
- Byte‑BPE \citep{radford2019gpt2}, character/byte models \citep{xue2022byt5, clark2022canine}

\subsection{Metrics}

- \textbf{Invertibility rate} (\% exact round‑trip) — must be 100\% for DST.
- \textbf{Sequence length} (avg tokens/sequence) — lower is better.
- \textbf{Entropy dispersion} (token entropy, n‑gram entropy) — lower indicates better compression.
- \textbf{Downstream} (perplexity/accuracy on domain tasks) — e.g., masked‑token prediction for code, next‑event prediction for logs.

\subsection{Setup}

We train vocabularies with comparable size budgets (e.g., 16k/32k). Models are lightweight Transformers (small/medium) trained for parity across tokenizers. We control for normalization differences and ensure byte coverage for fair comparison.

\subsection{Main Results}

\begin{table}[h]
  \centering
  \small
  \begin{tabular}{lcccc}
    \toprule
    Tokenizer & Invertible & Len. $\downarrow$ & Ent. $\downarrow$ & Downstream $\uparrow$ \\
    \midrule
    BPE &  \;no &  -- & -- & -- \\
    WordPiece & \;no & -- & -- & -- \\
    Unigram & \;no & -- & -- & -- \\
    Byte‑BPE &  yes & -- & -- & -- \\
    DST (ours) &  yes & -- & -- & -- \\
    \bottomrule
  \end{tabular}
  \caption{Summary across domains. Numbers to be filled after training; DST is guaranteed invertible and targets lower length/entropy.}
  \label{tab:main}
\end{table}

\subsection{Ablations}

- Effect of grammar priors (on/off)
- Max n‑gram $N$ and threshold $\theta$
- Byte fallback toggling and coverage of rare symbols

\subsection{Case Studies}

Qualitative examples show DST respecting code identifiers and delimiters, preventing leakage across tokens and enabling faithful de‑identification/restoration. For biological strings, fixed‑k k‑mers provide stable compression while maintaining invertibility.

\subsection{Reproducibility}

We will release scripts to reproduce vocab induction and compile DFST tokenizers. Artifacts export to \texttt{tokenizer.json} for immediate use with common LLM toolchains.
